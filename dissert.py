# -*- coding: utf-8 -*-
"""dissert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A-7ve9HARBVD761heBjtzolnXaDhhiy8
"""

#!pip install tldextract
#!pip install NGram
#!pip install ngram
#!pip install python-whois
#!pip install tkinter
import pandas as pd
import ipaddress
from urllib.parse import urlparse,urljoin
from urllib.parse import urlsplit
from urllib.parse import quote
import tldextract
import re
import whois
import os
import hashlib
from ngram import NGram
from datetime import datetime
import socket
import requests
import ipaddress
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

from flask import Flask, render_template, request
import pickle

# Load the model (for demonstration, this is a placeholder; replace with your model's path)
#URL_check = input("enter the requwired URL: ")
#url = URL_check

#filepath = "/content/drive/MyDrive/Colab Notebooks/Benign_list_big_final.xlsx"
filepath = "Benign_list_big_final.xlsx"

#TRUSTED_DOMAINS_HASH_FILE = "/content/drive/MyDrive/Colab Notebooks/Hashed_Trusted_File.xlsx"
TRUSTED_DOMAINS_HASH_FILE = "Hashed_Trusted_File.xlsx"

#filepath1 = "/content/drive/MyDrive/Colab Notebooks/Benign_list_big_final.xlsx"
filepath1 = "Benign_list_big_final.xlsx"

#trusted_domains_file = "/content/drive/MyDrive/Colab Notebooks/Hashed_Trusted_File.xlsx"
trusted_domains_file = "Hashed_Trusted_File.xlsx"





def getDomain(url):
    hostname = urlparse(url).hostname
    return hostname


def is_ipaddress(url):
    parsed_url = urlparse(url)
    netloc = parsed_url.netloc

    try:
        ipaddress.ip_address(netloc)
        return 1
    except ValueError:
        return 0

#check for the presence of '@' in the URL
def is_atInurl(url):
    parsed_url = urlparse(url)
 #   netloc = parsed_url.netloc.decode('utf-8')
    netloc = parsed_url.netloc
    if '@' in netloc:
      return 1
    else:
     return 0

#check for the presence of '*' in the URL
def is_asterikinpath(url):

    parsed_url = urlparse(url)
    path = parsed_url.path
    if '*' in path:
      return 1
    else:
      return 0

#check for the precense of '$' in the URL
def is_dollarinpath(url):
    split_url = urlsplit(url)
    path = split_url.path + split_url.query
    if '$' in path:
      return 1
    else:
      return 0

#check for the presence of "|" in the URL
def is_pipeinurl(url):
    split_url = urlsplit(url)
    path = split_url.path + split_url.query
    if '|' in path:
     return 1
    else:
      return 0

#check for the tld position
def check_tld_position(url):
    try:

        url_parts = tldextract.extract(url)

        # Check if the TLD appears in the subdomain
        if url_parts.suffix in url_parts.subdomain:
            return 1  # TLD is out of position
        else:
            return 0  # TLD is not out of position

    except Exception:
        return 1  # Other exceptions occurred


#check for the presence of "//"
def check_double_slash(url):
    parsed_url = urlparse(url)
    path = parsed_url.path

    if '//' in path:
        return 1
    else:
        return 0


#check for typo squatting
def typo_squatting(url, known_urls, threshold=0.8):
    def create_ngrams(url, n=3):
        return NGram([url], N=n)

    url_ngram = create_ngrams(url)
    for known_url in known_urls:
        known_url_ngram = create_ngrams(known_url)
        similarity = url_ngram.compare(url, known_url)
        if similarity == 1:
            return 0
        elif similarity >= threshold:
            return 1
    return 0

# Load the dataset of known URLs
known_urls_df = pd.read_excel('Benign_list_big_final.xlsx')
known_urls = known_urls_df['URLs'].tolist()

#check for the presence of url shortening
shortening_services = r"bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|" \
                      r"yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|" \
                      r"short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|" \
                      r"doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|db\.tt|" \
                      r"qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|q\.gs|is\.gd|" \
                      r"po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|x\.co|" \
                      r"prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|" \
                      r"tr\.im|link\.zip\.net"

def is_url_shortened(url):
    match=re.search(shortening_services,url)
    if match:
        return 1
    else:
        return 0

#check for the presence of "-" in URL
def is_dash_inDomain(url):

    parsed_url = urlparse(url)
    netloc = parsed_url.netloc
    if '-' in netloc:
     return 1
    else:
     return 0

#check for the number of dots in the URL
def check_number_of_dots(url):
    domain = url.split("//", 1)[-1].split("/", 1)[0]
    dot_count = domain.count('.')
    if dot_count > 3:
        return 1
    else:
        return 0

#check for status bar manipulation
def check_fake_status_bar(url):
    try:

        response = requests.get(url)


        source_code = response.text
        if 'window.status =' in source_code or 'window.defaultStatus =' in source_code:
            return 1  # Fake status bar modification detected
        else:
            return 0  # No fake status bar modification detected

    except requests.exceptions.RequestException:
        return 1  # Error occurred during the request

    except Exception:
        return 1  # Other exceptions occurred

#check for google index
def is_url_indexed(url):
    google_search_url = "https://www.google.com/search?q=site:" + url
    response = requests.get(google_search_url)
    soup = BeautifulSoup(response.content, "html.parser")

    return 0 if not soup.find(string="This site is not included in Google Search") else 1

#check login form manipulations
def check_login_form_action(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Get the base URL for resolving relative URLs
        base_url = urljoin(url, '/')

        for form in soup.find_all('form'):
            # Assume the form is a login form if it has an input element of type 'password'
            if form.find('input', attrs={'type': 'password'}):
                action = form.get('action')
                if action is None:
                    return 1  # The action attribute is null
                else:
                    action_url = urljoin(base_url, action)
                    if urlparse(action_url).netloc != urlparse(url).netloc:
                        return 1  # The form action is on a different domain
            else:
                return 0  # The form is not a login form or the action is on the same domain
        return 0  # No suspicious login forms found
    except requests.exceptions.RequestException:
        return 1  # An error occurred while trying to fetch the webpage


#check for hyperlinks manipulation
def hash_url(url):
    return hashlib.sha256(url.encode()).hexdigest()


def load_or_create_hashed_trusted_domains(filepath1):
    filepath1 = "Hashed_Trusted_File.xlsx"
    if os.path.exists(TRUSTED_DOMAINS_HASH_FILE):
        # Load hashed trusted domains from file
        with open(TRUSTED_DOMAINS_HASH_FILE, 'rb') as f:
            hashed_domains = pickle.load(f)
    else:
        # Compute hashed trusted domains and save to file
        hashed_domains = create_hashed_trusted_domains(filepath1)
        with open(TRUSTED_DOMAINS_HASH_FILE, 'wb') as f:
            pickle.dump(hashed_domains, f)
    return hashed_domains

# Use the function to load or create hashed trusted domains
hashed_trusted_domains = load_or_create_hashed_trusted_domains(trusted_domains_file)



#this code hashes each URL in the trusted url file
def create_hashed_trusted_domains(filepath1):
    hashed_domains = set()
    df = pd.read_excel(filepath1)
    for index, row in df.iterrows():
        hashed_domains.add(hash_url(row['URLs']))
    return hashed_domains

def check_hyperlinks(url, hashed_trusted_domains):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')

        domain = tldextract.extract(url).registered_domain
        links = soup.find_all('a')

        suspicious_keywords = ["login", "signin", "payment", "bank"]

        external_domain_count = 0
        for link in links:
            href = link.get('href')
            if href and hash_url(tldextract.extract(href).registered_domain) not in hashed_trusted_domains:
                if any(keyword in href for keyword in suspicious_keywords):
                    external_domain_count += 1

        if len(links) == 0 or external_domain_count > 0.8 * len(links):
            return 1
        else:
            return 0

    except requests.exceptions.RequestException:
        return 1

    except Exception:
        return 1

#check for iframe
def check_iframe_no_border(url):
    try:
        response = requests.get(url)
    except requests.exceptions.RequestException as e:
        print(f"Network error occurred while trying to reach {url}: {e}")
        return -1

    try:
        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all <iframe> tags
        iframe_tags = soup.find_all('iframe')

        for iframe in iframe_tags:
            # Check if the iframe is hidden using CSS
            style = iframe.get('style')
            if style and 'display: none' in style:
                return 1  # Invisible <iframe> tag exists

        return 0  # Invisible <iframe> tag does not exist

    except Exception as e:
        print("Error occurred while parsing the HTML: ", e)
        return -1

#check if righ-click is disabled
def check_right_click_disabled(url):
    try:
        # Send a GET request to retrieve the HTML content
        response = requests.get(url)
    except requests.exceptions.RequestException as e:
        print(f"Network error occurred while trying to reach {url}: {e}")
        return -1  # Return a different value or handle the exception as desired

    try:
        # Check if the source code contains patterns indicating right-click disabling
        source_code = response.text.lower()
        if "event.button == 2" in source_code or "oncontextmenu" in source_code:
            return 1  # Right-click is disabled
        else:
            return 0  # Right-click is not disabled
    except Exception as e:
        print(f"Error occurred while checking for right-click disabling: {e}")
        return 1  # Return a different value or handle the exception as desired


#check redirect
def check_redirects_with_timeout(url):
    try:
        response = requests.get(url, allow_redirects=True, timeout=10)  # 10 seconds timeout
        redirect_count = len(response.history)
        if response.status_code == 404:
            return 1
        else:
            if redirect_count >= 2:
                return 1
            else:
                return 0
    except requests.exceptions.RequestException as e:
        print("Error in check_redirect:", e)
        return 1

#This code checks the expiry of a domain
def domainEnd(url):
   domain_name = url.split("//")[-1].split("/")[0]
   w = whois.whois(domain_name)
   creation_date = w.creation_date
   expiration_date = w.expiration_date
   if (isinstance(creation_date,str) or isinstance(expiration_date,str)):
    try:
      creation_date = datetime.strptime(creation_date,'%Y-%m-%d')
      expiration_date = datetime.strptime(expiration_date,"%Y-%m-%d")
    except:
      return 1
   if ((expiration_date is None) or (creation_date is None)):
      return 1
   elif ((type(expiration_date) is list) or (type(creation_date) is list)):
      return 1
   else:
    ageofdomain = abs((expiration_date - creation_date).days)
    if ((ageofdomain/30) < 6):
      age = 1
    else:
      age = 0
   return age


#feature extraction
def features_extraction(url):

    features = []
#    features.append(getDomain(url))
    features.append(is_ipaddress(url))
    features.append(is_atInurl(url))
    features.append(is_asterikinpath(url))
    features.append(is_dollarinpath(url))
    features.append(is_pipeinurl(url))
    features.append(check_tld_position(url))
    features.append(check_double_slash(url))
    features.append(typo_squatting(url, known_urls, threshold=0.8))
    features.append(is_url_shortened(url))
    features.append(is_dash_inDomain(url))
    features.append(check_number_of_dots(url))


   # features.append(1 if dns == 1 else check_domain_existence(url))

    try:
       response = requests.get(quote(url))
    except:
      response = ""
    features.append(check_fake_status_bar(url))
    features.append(is_url_indexed(url))
    features.append(check_login_form_action(url))
    features.append(check_hyperlinks(url, TRUSTED_DOMAINS_HASH_FILE))
    features.append(check_iframe_no_border(url))
    features.append(check_right_click_disabled(url))
    features.append(check_redirects_with_timeout(url))

#check if domain exists
    dns = 0
    try:
     domain_name = whois.whois(urlparse(url).netloc)
    except:
      dns = 1
    features.append(dns)

    features.append(1 if dns == 1 else domainEnd(url))


    return tuple(features)

#creating a dataframe

